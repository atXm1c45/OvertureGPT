{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608217ff",
      "metadata": {
        "id": "608217ff",
        "outputId": "b9f51a1e-71c0-43c8-c816-05468617e23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Cleaning up old files...\n",
            "‚¨áÔ∏è Cloning repository...\n",
            "Cloning into 'VideoLLaMA2'...\n",
            "remote: Enumerating objects: 986, done.\u001b[K\n",
            "remote: Counting objects: 100% (427/427), done.\u001b[K\n",
            "remote: Compressing objects: 100% (182/182), done.\u001b[K\n",
            "remote: Total 986 (delta 361), reused 245 (delta 245), pack-reused 559 (from 2)\u001b[K\n",
            "Receiving objects: 100% (986/986), 55.81 MiB | 39.77 MiB/s, done.\n",
            "Resolving deltas: 100% (661/661), done.\n",
            "/content/VideoLLaMA2\n",
            "Branch 'audio_visual' set up to track remote branch 'audio_visual' from 'origin'.\n",
            "Switched to a new branch 'audio_visual'\n",
            "üõ†Ô∏è Patching for Colab 2025...\n",
            "ü©π Applied Numpy 2.0 Patch\n",
            "üì¶ Installing dependencies (Keep existing versions)...\n",
            "Obtaining file:///content/VideoLLaMA2\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: videollama2\n",
            "  Building editable for videollama2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for videollama2: filename=videollama2-1.0-0.editable-py3-none-any.whl size=13152 sha256=be3e9cf5da6c5600199bdf7e59b9c871d7b653b0af895264849aa2337050fb20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-95larnc1/wheels/c0/86/ae/82d555d878ad114b123553e8ebc685ac5ea31e023bfb9d4d33\n",
            "Successfully built videollama2\n",
            "Installing collected packages: videollama2\n",
            "  Attempting uninstall: videollama2\n",
            "    Found existing installation: videollama2 1.0\n",
            "    Uninstalling videollama2-1.0:\n",
            "      Successfully uninstalled videollama2-1.0\n",
            "Successfully installed videollama2-1.0\n",
            "/content\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:54: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ SUCCESS: Setup Verified. The crash is fixed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "# 1. CLEANUP (Wipe previous failed attempts)\n",
        "if os.path.exists(\"VideoLLaMA2\"):\n",
        "    print(\"üßπ Cleaning up old files...\")\n",
        "    shutil.rmtree(\"VideoLLaMA2\")\n",
        "\n",
        "# 2. CLONE\n",
        "print(\"‚¨áÔ∏è Cloning repository...\")\n",
        "!git clone https://github.com/DAMO-NLP-SG/VideoLLaMA2.git\n",
        "%cd VideoLLaMA2\n",
        "!git checkout audio_visual\n",
        "\n",
        "# 3. THE FIX: Remove ALL version constraints\n",
        "# We tell the AI: \"Use whatever versions Colab already has.\"\n",
        "print(\"üõ†Ô∏è Patching for Colab 2025...\")\n",
        "\n",
        "# Remove Numpy & Torch constraints entirely (Fixes the binary crash)\n",
        "!sed -i '/numpy/d' pyproject.toml\n",
        "!sed -i '/numpy/d' requirements.txt\n",
        "!sed -i '/torch/d' requirements.txt\n",
        "!sed -i '/torchvision/d' requirements.txt\n",
        "\n",
        "# Remove Flash Attention (Incompatible with T4)\n",
        "!sed -i '/flash-attn/d' pyproject.toml\n",
        "!sed -i '/flash-attn/d' requirements.txt\n",
        "\n",
        "# 4. COMPATIBILITY PATCH (Monkey Patch)\n",
        "# Numpy 2.0 removed 'np.float'. We add it back so old code doesn't break.\n",
        "import numpy as np\n",
        "if not hasattr(np, 'float'):\n",
        "    np.float = float\n",
        "    print(\"ü©π Applied Numpy 2.0 Patch\")\n",
        "\n",
        "# 5. INSTALL\n",
        "print(\"üì¶ Installing dependencies (Keep existing versions)...\")\n",
        "# We use --no-deps for the library itself to prevent it from downgrading anything\n",
        "!pip install -r requirements.txt --quiet\n",
        "!pip install ffmpeg-python bitsandbytes accelerate deepspeed --quiet\n",
        "!pip install -e . --no-build-isolation --no-deps\n",
        "\n",
        "# 6. VERIFY\n",
        "%cd ..\n",
        "sys.path.append(\"/content/VideoLLaMA2\")\n",
        "try:\n",
        "    from videollama2 import model_init, mm_infer\n",
        "    print(\"\\n‚úÖ SUCCESS: Setup Verified. The crash is fixed.\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\n‚ùå ERROR: {e}\")\n",
        "except ValueError as e:\n",
        "    print(f\"\\n‚ùå VERSION ERROR: {e}\")\n",
        "    print(\"Did you forget to 'Restart Session' before running this?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7464bbf5",
      "metadata": {
        "id": "7464bbf5",
        "outputId": "cdeacd23-7e53-40f1-9004-13edec2c0879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Downloading Model (Ultra-Light 4-bit Mode)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "transformers.modeling_utils.PreTrainedModel.from_pretrained() got multiple values for keyword argument 'low_cpu_mem_usage'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3173030702.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 3. Load with 4-bit Compression (The \"Anti-Crash\" setting)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# This forces the model to take up minimal space.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m model, processor, tokenizer = model_init(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# <--- CHANGED TO 4-BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VideoLLaMA2/videollama2/__init__.py\u001b[0m in \u001b[0;36mmodel_init\u001b[0;34m(model_path, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DAMO-NLP-SG/VideoLLaMA2-7B\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_name_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VideoLLaMA2/videollama2/model/__init__.py\u001b[0m in \u001b[0;36mload_pretrained_model\u001b[0;34m(model_path, model_base, model_name, load_8bit, load_4bit, device_map, device, use_flash_attn, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideollama2MixtralForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'videollama2_qwen2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideollama2Qwen2ForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'videollama2_gemma2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideollama2Gemma2ForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: transformers.modeling_utils.PreTrainedModel.from_pretrained() got multiple values for keyword argument 'low_cpu_mem_usage'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "from videollama2 import model_init, mm_infer\n",
        "from videollama2.utils import disable_torch_init\n",
        "\n",
        "# 1. Clear RAM before loading\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 2. Define Path\n",
        "model_path = \"DAMO-NLP-SG/VideoLLaMA2.1-7B-AV\"\n",
        "print(\"‚è≥ Downloading Model (4-bit Mode)...\")\n",
        "\n",
        "disable_torch_init()\n",
        "\n",
        "# 3. Load Model\n",
        "# I removed 'low_cpu_mem_usage=True' because the library adds it automatically.\n",
        "model, processor, tokenizer = model_init(\n",
        "    model_path,\n",
        "    load_in_4bit=True,  # This keeps it small (approx 5GB)\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model Loaded & Ready!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}