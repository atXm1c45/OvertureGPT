{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608217ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up old files...\n",
      "‚¨áÔ∏è Cloning repository...\n",
      "Cloning into 'VideoLLaMA2'...\n",
      "remote: Enumerating objects: 986, done.\u001b[K\n",
      "remote: Counting objects: 100% (427/427), done.\u001b[K\n",
      "remote: Compressing objects: 100% (182/182), done.\u001b[K\n",
      "remote: Total 986 (delta 361), reused 245 (delta 245), pack-reused 559 (from 2)\u001b[K\n",
      "Receiving objects: 100% (986/986), 55.81 MiB | 40.22 MiB/s, done.\n",
      "Resolving deltas: 100% (661/661), done.\n",
      "/content/VideoLLaMA2\n",
      "Branch 'audio_visual' set up to track remote branch 'audio_visual' from 'origin'.\n",
      "Switched to a new branch 'audio_visual'\n",
      "üõ†Ô∏è Patching for Colab 2025...\n",
      "üì¶ Installing dependencies (Keep existing versions)...\n",
      "Obtaining file:///content/VideoLLaMA2\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: videollama2\n",
      "  Building editable for videollama2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for videollama2: filename=videollama2-1.0-0.editable-py3-none-any.whl size=13152 sha256=7e2e078e4038988219768511598f83d6a97e9271ed48db130ccf2d7bf43445c3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xw6ma8db/wheels/c0/86/ae/82d555d878ad114b123553e8ebc685ac5ea31e023bfb9d4d33\n",
      "Successfully built videollama2\n",
      "Installing collected packages: videollama2\n",
      "  Attempting uninstall: videollama2\n",
      "    Found existing installation: videollama2 1.0\n",
      "    Uninstalling videollama2-1.0:\n",
      "      Successfully uninstalled videollama2-1.0\n",
      "Successfully installed videollama2-1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e978ec3879164c9181523e0bc277a0c0",
       "pip_warning": {
        "packages": [
         "videollama2"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "\n",
      "‚úÖ SUCCESS: Setup Verified. The crash is fixed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# 1. CLEANUP (Wipe previous failed attempts)\n",
    "if os.path.exists(\"VideoLLaMA2\"):\n",
    "    print(\"üßπ Cleaning up old files...\")\n",
    "    shutil.rmtree(\"VideoLLaMA2\")\n",
    "\n",
    "# 2. CLONE\n",
    "print(\"‚¨áÔ∏è Cloning repository...\")\n",
    "!git clone https://github.com/DAMO-NLP-SG/VideoLLaMA2.git\n",
    "%cd VideoLLaMA2\n",
    "!git checkout audio_visual\n",
    "\n",
    "# 3. THE FIX: Remove ALL version constraints\n",
    "# We tell the AI: \"Use whatever versions Colab already has.\"\n",
    "print(\"üõ†Ô∏è Patching for Colab 2025...\")\n",
    "\n",
    "# Remove Numpy & Torch constraints entirely (Fixes the binary crash)\n",
    "!sed -i '/numpy/d' pyproject.toml\n",
    "!sed -i '/numpy/d' requirements.txt\n",
    "!sed -i '/torch/d' requirements.txt\n",
    "!sed -i '/torchvision/d' requirements.txt\n",
    "\n",
    "# Remove Flash Attention (Incompatible with T4)\n",
    "!sed -i '/flash-attn/d' pyproject.toml\n",
    "!sed -i '/flash-attn/d' requirements.txt\n",
    "\n",
    "# 4. COMPATIBILITY PATCH (Monkey Patch)\n",
    "# Numpy 2.0 removed 'np.float'. We add it back so old code doesn't break.\n",
    "import numpy as np\n",
    "if not hasattr(np, 'float'):\n",
    "    np.float = float\n",
    "    print(\"ü©π Applied Numpy 2.0 Patch\")\n",
    "\n",
    "# 5. INSTALL\n",
    "print(\"üì¶ Installing dependencies (Keep existing versions)...\")\n",
    "# We use --no-deps for the library itself to prevent it from downgrading anything\n",
    "!pip install -r requirements.txt --quiet\n",
    "!pip install ffmpeg-python bitsandbytes accelerate deepspeed --quiet\n",
    "!pip install -e . --no-build-isolation --no-deps\n",
    "\n",
    "# 6. VERIFY\n",
    "%cd ..\n",
    "sys.path.append(\"/content/VideoLLaMA2\") \n",
    "try:\n",
    "    from videollama2 import model_init, mm_infer\n",
    "    print(\"\\n‚úÖ SUCCESS: Setup Verified. The crash is fixed.\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå ERROR: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n‚ùå VERSION ERROR: {e}\")\n",
    "    print(\"Did you forget to 'Restart Session' before running this?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7464bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Downloading Model (4-bit Mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-482735853.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 3. Load Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# I removed 'low_cpu_mem_usage=True' because the library adds it automatically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m model, processor, tokenizer = model_init(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# This keeps it small (approx 5GB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/VideoLLaMA2/videollama2/__init__.py\u001b[0m in \u001b[0;36mmodel_init\u001b[0;34m(model_path, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DAMO-NLP-SG/VideoLLaMA2-7B\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_name_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/VideoLLaMA2/videollama2/model/__init__.py\u001b[0m in \u001b[0;36mload_pretrained_model\u001b[0;34m(model_path, model_base, model_name, load_8bit, load_4bit, device_map, device, use_flash_attn, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideollama2MixtralForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'videollama2_qwen2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideollama2Qwen2ForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'videollama2_gemma2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideollama2Gemma2ForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3279\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3280\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU found. A GPU is needed for quantization.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from videollama2 import model_init, mm_infer\n",
    "from videollama2.utils import disable_torch_init\n",
    "\n",
    "# 1. Clear RAM before loading\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 2. Define Path\n",
    "model_path = \"DAMO-NLP-SG/VideoLLaMA2.1-7B-AV\"\n",
    "print(\"‚è≥ Downloading Model (4-bit Mode)...\")\n",
    "\n",
    "disable_torch_init()\n",
    "\n",
    "# 3. Load Model\n",
    "# I removed 'low_cpu_mem_usage=True' because the library adds it automatically.\n",
    "model, processor, tokenizer = model_init(\n",
    "    model_path, \n",
    "    load_in_4bit=True,  # This keeps it small (approx 5GB)\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model Loaded & Ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
